<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="A reinforcement learning framework where LLMs generate their own supervision signal via majority voting">
  <meta name="keywords" content="LLM, Reasoning, Self-Improvement">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>srt</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation" style="background-color: #333; color: #fff;">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false" style="color: #fff;">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="#abstract" style="color: #fff; border-bottom: 0px solid #fff;">
        Abstract
      </a>
      <a class="navbar-item" href="#setup" style="color: #fff; border-bottom: 0px solid #fff;">
        Problem Setup
      </a>
      <a class="navbar-item" href="#srt" style="color: #fff; border-bottom: 0px solid #fff;">
        SRT
      </a>
      <a class="navbar-item" href="#empirical" style="color: #fff; border-bottom: 0px solid #fff;">
        Empirical Results
      </a>
      <a class="navbar-item" href="#BibTeX" style="color: #fff; border-bottom: 0px solid #fff;">
        BibTeX
      </a>
  </div>
</nav>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Can Large Reasoning Models Self-Train?</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://sheikhshafayat.github.io/">Sheikh Shafayat</a><sup>*1</sup>,
            </span>
            <span class="author-block">
              <a href="https://tajwarfahim.github.io/">Fahim Tajwar</a><sup>*2</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.cs.cmu.edu/~rsalakhu/">Ruslan Salakhutdinov</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.cs.cmu.edu/~schneide/">Jeff Schneider</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://azanette.com">Andrea Zanette</a><sup>2</sup>
            </span>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>*</sup>Equal Contribution</span>
            <span class="author-block"><sup>1</sup>Independent Researcher</span>
            <span class="author-block"><sup>2</sup>Carnegie Mellon University</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/abs/2502.17543"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/tajwarfahim/srt"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <span class="link-block">
                <a href="https://huggingface.co/collections/ftajwar/self-rewarding-llm-training-6835218091832c3664176553"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-database"></i>
                  </span>
                  <span>Datasets</span>
                  </a>
              </span>

          </div>
        </div>
      </div>
    </div>
  </div>

</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/srt_teaser_figure.png"
        alt="Teaser image."
        class="teaser-image"/>
      <h2 class="subtitle has-text-centered">
        <b>(Overview of Self-Rewarding Training or SRT)</b> In Reinforcement Learning from Verifiable Reward (RLVR), one produces the reward for RL training using a ground truth verifier. 
        Contrary to that, Self-Rewarding Training (SRT) does not assume access to a ground truth verifier; instead it uses majority voting 
        from the model's own generations to estimate the ground truth, and use this proxy reward signal to train the model.
      </h2>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3" id="abstract">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Scaling the performance of large language models (LLMs) increasingly depends on 
            methods that reduce reliance on human supervision. 
            Reinforcement learning from automated verification offers an alternative, but it incurs scalability limitations 
            due to dependency upon human-designed verifiers. Self-training, where the model’s own judgment provides the 
            supervisory signal, presents a compelling direction. 
            We propose an online self-training reinforcement learning algorithm that leverages the model’s self-consistency 
            to infer correctness signals and train without any ground-truth supervision. 
            We apply the algorithm to challenging mathematical reasoning tasks and show that it quickly reaches performance 
            levels rivaling reinforcement-learning methods trained explicitly on gold-standard answers. 
            Additionally, we analyze inherent limitations of the algorithm, highlighting how the self-generated proxy reward 
            initially correlated with correctness can incentivize reward hacking, where confidently 
            incorrect outputs are favored. Our results illustrate how self-supervised improvement can achieve significant performance 
            gains without external labels, while also revealing its fundamental challenges.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  <hr>
<section class="section">
  <div class="container is-max-desktop">
    <!-- Animation. -->
    <div class="columns is-centered">
      <div class="column is-full-width">

        <h2 class="title is-3" id="setup">Problem Setting</h2>
        <div class="content has-text-justified">
          <p>
            Recently reinforcement learning from verifiable rewards (RLVR) have seen a lot of success at improving
            LLMs' reasoning abilities, particularly in the domains like math and coding (<a href="https://arxiv.org/abs/2412.16720">OpenAI et al.</a>,
            <a href="https://arxiv.org/abs/2501.12948">DeepSeek-AI et al.</a>). 
            However, these approaches 
            require ground truth (possibly human written) verifiers to provide reward signal during training. This can
            be quite limiting, since collecting gold answers for every problem we care about can become expensive.
          </p>

          <p>
            The goal of our paper is to answer the following questions: 
          </p>
          <ul>
            <li><b>(1)</b> Can LLMs generate their own training supervision? If so, can we provide a simple algorithm for this?</li>
            <li><b>(2)</b> How does this algorithm for self-training perform?</li>
            <li><b>(3)</b> Do this method have any limitations? How can we address them?</li>
          </ul>
        </div>

        <h2 class="title is-3" id="srt">Self-Rewarding Training (SRT)</h2>
        <div class="content has-text-justified">
          In our work, we provide a simple algorithm for self-training. We notice that LLMs are generally more consistent when
          they are correct (for example, majority voting typically outperforms average performance). We leverage this property
          to generate training signal for an LLM wihout ground truth labels --- concretely, during RL training, we calculate
          the majority voting answer for each prompt, and use this as if it was the ground truth answer. This is a simple algorithm
          compatible with any RLVR algorithm since the only change is how we generate the reward signal. Next, we describe different
          components of SRT.
        </div>

        <center>
          <img src="./static/images/srt_components.png"
            width="60%"
            alt="srt_components"
            class="srt_components"/>
        </center>

        <div class="box" style="background-color:aliceblue">
          <div class="content has-text-centered">
            <p>
              <b>(Different Components of SRT)</b> The SRT algorithm consists of taking an prompt or question,
              generating multiple responses from the LLM for the prompt, calculate the majority voting answer based on these
              responses, and then use the majority voting answer as a proxy for ground truth to provide reward for RL training.
            </p>
          </div>
        </div>
        
        <h3 class="title is-4">Training Algorithm</h3>
        <div class="content has-text-justified">
          <!-- Create Bullet Point List to Summarize -->
          We provide our formal training algorithm below:
        </div>

        <center>
          <img src="./static/images/srt_algorithm.png"
            width="50%"
            alt="srt_algorithm"
            class="srt_algorithm"/>
        </center>        

        <h2 class="title is-3" id="empirical">Empirical Results</h2>

        <!-- Interpolating. -->
        <h3 class="title is-4">SRT leads to 100% relative improvement, but also suffers from performance collapse</h3>
        
        <center>
          <img src="./static/images/self_training_performance.png"
            alt="self_training_performance"
            width="90%"
            class="self_training_performance"/>
        </center>

        <div class="box" style="background-color:aliceblue">
          <div class="content has-text-centered">
            <p>
              <b>(Self-Training Performance)</b> Average over 3 heldout test dataset performance of SRT vs training with ground truth reward. 
              Training for longer with SRT can lead to performance collapse, but at the peak, it leads to around 100\% performance improvement over the base model.
              Earlier in the training, it leads to similar improvement as RL training with ground truth.
            </p>
          </div>
        </div>

        <h3 class="title is-4">PAPRIKA can teach LLMs strategies that generalize zero-shot to a new task group</h3>

        The next important question we study is whether the strategies learned by PAPRIKA can zero-shot transfer to entirely different groups of tasks. To do so, we perform a set of leave-one-out (LOO) experiments: we randomly choose one group (e.g., 20 questions) from our set of environments, train the LLM on trajectories generated from every other group, and test the resulting model's performance on the left-out group. Our results show that PAPRIKA (LOO) outperforms the starting model, Llama-3.1-8B-Instruct, in 6 representative task groups. This shows that the strategic exploration taught by PAPRIKA is not tied to a particular environment, and <b>scaling up the number of task groups in PAPRIKA can be a viable solution for teaching LLMs general in-context RL abilities</b>.
        
        <center>
          <img src="./static/images/paprika_generalization.png"
            alt="paprika_generalization"
            width="90%"
            class="paprika_generalization"/>
        </center>

        <div class="box" style="background-color:aliceblue">
          <div class="content has-text-centered">
            <p>
              <b>(Testing generalization of PAPRIKA via leave-one-out experiments)</b> We test PAPRIKA's zero-shot performance on unseen task groups by leave-one-out (LOO) experiments, where we train the LLM on trajectories from every task group except the group we test on. All evaluations are done at temperature 0.7 and we report <b>average success rate</b>. Our experiments demonstrate that PAPRIKA can teach an LLM sequential decision making abilities that often transfers well to new tasks without needing any additional training.
            </p>
          </div>
        </div>
  
        <h3 class="title is-4">Curriculum Learning Can Improve PAPRIKA's Sample Complexity</h3>

        We test our curriculum learning algorithm on 20 questions, using GPT-4o-mini defined easy, medium, and hard task group clustering of the hidden topics. Our curriculum outperforms uniform sampling over 3 rounds of iterative training in terms of both average and pass@4 success rate, showcasing its potential for improving the sample complexity of such methods, <b>potentially also working with Online RL</b>.

        <center>
          <img src="./static/images/paprika_curriculum.png"
            alt="paprika_curriculum"
            width="90%"
            class="paprika_curriculum"/>
        </center>
        
        <div class="box" style="background-color:aliceblue">
          <div class="content has-text-justified">
            <p>
              <b>(Multi-round training with curriculum on twenty questions)</b> We demonstrate the efficacy of our curriculum learning algorithm for sampling training tasks by comparing its performance against uniform sampling for multi-round training. All evaluations are done at temperature 0.7, and shaded regions represent standard error over 3 seeds. (<b>Left</b>) Average success rate at each round. (<b>Middle</b>) Pass@4 success rate at each round. (<b>Right</b>) Success rate per each of easy, medium, and hard task groups. Overall, our curriculum learning algorithm shows 1.4% and 3.3% improvement over the uniform sampling baseline at average and pass@4 success rate respectively.
            </p>
          </div>
        </div>

        <h3 class="title is-4">Example Trajectories</h3>
        
        Here we list two example trajectories, one from the regular Llama-3.1-8B-Instruct model, and another from the PAPRIKA fine-tuned model, on the 20 questions task group. The hidden topic the agent has to guess is a <b>concept</b>, and the correct answer is <b>primary numbers</b>. We see qualitatively that PAPRIKA teaches the model to ask better quality questions.

        <center>
          <img src="./static/images/paprika_example_task_trajectories.png"
            alt="paprika_example_task_trajectories"
            width="70%"
            class="paprika_example_task_trajectories"/>
        </center>

        <div class="box" style="background-color:aliceblue">
          <div class="content has-text-justified">
            <p>
              <b>(Qualitative analysis of behaviors taught by PAPRIKA)</b> PAPRIKA not only improves over the starting model in terms of success rate and other metrics, but also shows better sequential decision making abilities. Here we present two example trajectories on 20 questions, where the hidden topic the agent has to guess is a <b>concept</b>, and the correct answer is <b>primary numbers</b>. The PAPRIKA fine-tuned model asks much better quality questions and is able to guess the topic in 8 turns, whereas the regular instruct model cannot guess it after spending all 20 turns, in all 4 attempts that we ran (we only show first 9 turns for the sake of brevity).
            </p>
          </div>
        </div>
        
      </div>
    </div>
  </div>
</section>

<hr>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code> 
      @misc{shafayat2025CanLargeReasoningModelsSelfTrain,
        title={Can Large Reasoning Models Self-Train?}, 
        author={Sheikh Shafayat and Fahim Tajwar and Ruslan Salakhutdinov and Jeff Schneider and Andrea Zanette},
        year={2025},
        url={https://self-rewarding-llm-training.github.io/}, 
      }
</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/pdf/2502.17543">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/tajwarfahim/paprika" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <p>
        Corresponding Author: <a href="mailto:sheikhshafayat2@gmail.com">Sheikh Shafayat</a>, <a href="mailto:ftajwar@cs.cmu.edu">Fahim Tajwar</a>, <a href="mailto:zanette@cmu.edu">Andrea Zanette</a>.<br>
        We thank the Nerfies Team for their <a href="https://github.com/nerfies/nerfies.github.io">website template</a>.
      </p>
    </div>
  </div>
</footer>

</body>
</html>
