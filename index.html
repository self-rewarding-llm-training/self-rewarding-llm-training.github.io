<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="A reinforcement learning framework where LLMs generate their own supervision signal via majority voting">
  <meta name="keywords" content="LLM, Reasoning, Self-Improvement">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>SRT: Can Large Reasoning Models Self-Train?</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation" style="background-color: #333; color: #fff;">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false" style="color: #fff;">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="#abstract" style="color: #fff; border-bottom: 0px solid #fff;">
        Abstract
      </a>
      <a class="navbar-item" href="#setup" style="color: #fff; border-bottom: 0px solid #fff;">
        Problem Setup
      </a>
      <a class="navbar-item" href="#srt" style="color: #fff; border-bottom: 0px solid #fff;">
        SRT
      </a>
      <a class="navbar-item" href="#empirical" style="color: #fff; border-bottom: 0px solid #fff;">
        Empirical Results
      </a>
      <a class="navbar-item" href="#BibTeX" style="color: #fff; border-bottom: 0px solid #fff;">
        BibTeX
      </a>
  </div>
</nav>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Can Large Reasoning Models Self-Train?</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://sheikhshafayat.github.io/">Sheikh Shafayat</a><sup>*</sup>,
            </span>
            <span class="author-block">
              <a href="https://tajwarfahim.github.io/">Fahim Tajwar</a><sup>*</sup>,
            </span><br>
            <span class="author-block">
              <a href="https://www.cs.cmu.edu/~rsalakhu/">Ruslan Salakhutdinov</a>,
            </span>
            <span class="author-block">
              <a href="https://www.cs.cmu.edu/~schneide/">Jeff Schneider</a>,
            </span>
            <span class="author-block">
              <a href="https://azanette.com">Andrea Zanette</a>
            </span>                      

          <div class="is-size-5 publication-authors">
            <span class="author-block">Carnegie Mellon University</span><br>
            <span class="author-block"><sup>*</sup>Equal Contribution</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/abs/2505.21444"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/tajwarfahim/srt"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <span class="link-block">
                <a href="https://huggingface.co/collections/ftajwar/self-rewarding-llm-training-6835218091832c3664176553"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-database"></i>
                  </span>
                  <span>Datasets</span>
                  </a>
              </span>

          </div>
        </div>
      </div>
    </div>
  </div>

</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3" id="abstract">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Scaling the performance of large language models (LLMs) increasingly depends on 
            methods that reduce reliance on human supervision. 
            Reinforcement learning from automated verification offers an alternative, but it incurs scalability limitations 
            due to dependency upon human-designed verifiers. Self-training, where the model’s own judgment provides the 
            supervisory signal, presents a compelling direction. 
            We propose an online self-training reinforcement learning algorithm that leverages the model’s self-consistency 
            to infer correctness signals and train without any ground-truth supervision. 
            We apply the algorithm to challenging mathematical reasoning tasks and show that it quickly reaches performance 
            levels rivaling reinforcement-learning methods trained explicitly on gold-standard answers. 
            Additionally, we analyze inherent limitations of the algorithm, highlighting how the self-generated proxy reward 
            initially correlated with correctness can incentivize reward hacking, where confidently 
            incorrect outputs are favored. Our results illustrate how self-supervised improvement can achieve significant performance 
            gains without external labels, while also revealing its fundamental challenges.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  <hr>


  <!-- Threshold Plot -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/srt_threshold_plot.png"
        alt="Teaser image."
        class="teaser-image"/>
      <h2 class="subtitle has-text-justified">
        <b>Motivation:</b>
        Language models make more correct predictions when their answers are ensembled via majority voting (majority vote accuracy)
        than when they make single predictions (average per prompt accuracy, orange line). The goal of this work is to see if we can leverage this signal to train language models without requiring ground-truth labels.


      </h2>
    </div>
  </div>
</section>

<!-- Threshold Plot -->

<section class="section">
  <div class="container is-max-desktop">
    <!-- Animation. -->
    <div class="columns is-centered">
      <div class="column is-full-width">

        <h2 class="title is-3" id="setup">Problem Setting</h2>
        <div class="content has-text-justified">
          <p>
            Reinforcement learning from verifiable rewards (RLVR) has demonstrated significant success in enhancing the reasoning capabilities of large language models (LLMs), particularly in domains such as mathematics and coding (<a href="https://arxiv.org/abs/2412.16720">OpenAI et al.</a>, <a href="https://arxiv.org/abs/2501.12948">DeepSeek-AI et al.</a>). 
            However, these approaches depend on ground-truth verifiers—often created by humans—to generate reward signals during training. This reliance can be restrictive, as obtaining gold-standard solutions for every relevant problem is frequently costly.
          </p>          
          <p>
            <p>
              In this paper, we ask the following questions:
            </p>
            <ul>
              <li><b>(1)</b> Can LLMs generate their own supervision for training? If so, can we propose a simple algorithm to achieve this?</li>
              <li><b>(2)</b> How effective is this self-training algorithm?</li>
              <li><b>(3)</b> Does this method have limitations, and if so, how might we overcome them?</li>
            </ul>            
        </div>

        <h2 class="title is-3" id="srt">Self-Rewarded Training (SRT)</h2>
        <div class="content has-text-justified">
            In this paper, we propose and investigate a simple self-training algorithm based on the observation that LLMs tend to be more consistent when their answers are correct—indeed, majority voting usually achieves better performance than individual answers. 
            We exploit this property to generate training signals without requiring ground-truth labels. 
            Specifically, during reinforcement learning, we calculate the majority-vote answer for each prompt and treat this consensus as the ground-truth label. 
            This straightforward approach is compatible with any RLVR method, as it only modifies how the reward signal is derived. 
            Below, we describe the key components of the Self-Rewarded Training (SRT) algorithm.
        </div>

        <!-- <center>
          <img src="./static/images/srt_components.png"
            width="60%"
            alt="srt_components"
            class="srt_components"/>
        </center> -->
        <section class="hero teaser">
          <div class="container is-max-desktop">
            <div class="hero-body">
              <img src="./static/images/srt_teaser_figure.png"
                alt="Teaser image."
                class="teaser-image"/>
              <h2 class="subtitle has-text-centered">
                <b>(Overview of Self-Rewarded Training)</b><br>Unlike reinforcement learning with verifiable rewards, which uses a ground-truth verifier for reward generation, Self-Rewarding Training estimates rewards intrinsically through majority voting among model-generated solutions, allowing continual self-improvement without external annotations.
              </h2>
            </div>
          </div>
        </section>

        <div class="box" style="background-color:aliceblue">
          <div class="content has-text-justified">
            <p>
              <b>Self-Rewarded Training: </b> The SRT algorithm involves taking a prompt or question, generating multiple responses from the LLM, calculating the majority-vote answer from these responses, and then using this majority-voted answer as a proxy for ground truth to provide a reward signal for reinforcement learning training.
            </p>
          </div>
        </div>
        
        <div class="content has-text-justified">
          <!-- Create Bullet Point List to Summarize -->
          We provide our formal training algorithm below.
        </div>

        <center>
          <img src="./static/images/srt_algorithm.png"
            width="50%"
            alt="srt_algorithm"
            class="srt_algorithm"/>
        </center>        

        <h2 class="title is-3" id="empirical">Empirical Results</h2>

        <!-- Interpolating. -->
        <!-- <h3 class="title is-4">SRT leads to 100% relative improvement, but also suffers from performance collapse</h3> -->
        
        <center>
          <img src="./static/images/self_training_performance.png"
            alt="self_training_performance"
            width="90%"
            class="self_training_performance"/>
        </center>

        <div class="box" style="background-color:aliceblue">
          <div class="content has-text-centered">
            <p>
              <b>(Self-Training Performance)</b><br> 
              Average performance of SRT versus RL training evaluated on three held-out test datasets using ground truth rewards. Early in training, SRT performance closely matches RL but collapses with prolonged training. At its peak, SRT achieves approximately 100% improvement over the base model.
            </p>
          </div>
        </div>

        <h3 class="title is-4">What happens after SRT peaks in performance?</h3>

        Prolonged training with SRT can lead to <b>reward hacking</b> ---the model 
        learns to produce consistent responses in order to optimize its self-assigned reward, 
        irrespective of their true correctness. Indeed, manual analysis of the model outputs 
        (examples shown below) confirms this hypothesis: after collapse, the model outputs a very high entropy, 
        essentially random, set of tokens followed by the same template final answer that is nearly independent of the input prompt.
        In other words, <b>the initially strong correlation between the SRT objective and correctness is ultimately compromised, 
        becoming no longer a reliable proxy signal.</b>
        
        <center>
          <img src="./static/images/srt_reward_hacking_example.png"
            alt="srt_reward_hacking_example"
            width="90%"
            class="srt_reward_hacking_example"/>
        </center>

        <div class="box" style="background-color:aliceblue">
          <div class="content has-text-centered">
            <p>
              <b>(Model response for the same prompts before and after training with SRT)</b> Three examples of model responses 
              for the same prompt, before and after prolonged training with SRT on the DAPO dataset, 
              for a Qwen2.5-Math-7B model. Notice that for some prompts, the model responses before training ends before completion, 
              this is due to the model running out of our token generation budget. The model after 1200 steps of SRT training 
              exhibits performance collapse, and it outputs \boxed{1} and some other incoherent set of tokens 
              irrespective of the given prompt.
            </p>
          </div>
        </div>

        <center>
          <img src="./static/images/srt_training_dynamics.png"
            alt="srt_training_dynamics"
            width="100%"
            class="srt_training_dynamics"/>
        </center>

        <div class="box" style="background-color:aliceblue">
          <div class="content has-text-centered">
            <p>
              <b>(Self-Training Dynamics)</b> Extended training via SRT can lead to reward hacking, 
              as demonstrated by the sudden hike in KL penalty and training (psuedo-)reward, but collapse of accuracy on the held-out test sets.
            </p>
          </div>
        </div>
  
        <h3 class="title is-4">SRT can be used for test-time training</h3>

        Since SRT does not require ground truth answers, one can also use our algorithm for test-time training —
        that is, given a test set, one can first run SRT on this set, and then make predictions. We
        see that this can lead to minor
        performance improvement on the test set.

        <center>
          <img src="./static/images/srt_test_time_training.png"
            alt="srt_test_time_training"
            width="100%"
            class="srt_test_time_training"/>
        </center>
        
        <div class="box" style="background-color:aliceblue">
          <div class="content has-text-justified">
            <p>
              <b>(Test-Time Self-Training Performance)</b> Given the test dataset, 
              one can perform SRT on this dataset before making predictions. Our results show that this 
              improves the majority voting performance on the test set without access to ground truth labels.
            </p>
          </div>
        </div>

        Interstingly, we observe very little reward hacking or performance collapse in the test-time training setup, compared to the regular setup.
        Our analysis suggests that given the test dataset is small, SRT pseudo-reward can very quickly saturate (see second plot in the figure below), 
        which means very little learning signal is provided to the model throughout the rest of the training and hence, the model does not get the chance to learn to hack the reward.

        <center>
          <img src="./static/images/srt_test_time_training_dynamics.png"
            alt="srt_test_time_training_dynamics"
            width="100%"
            class="srt_test_time_training_dynamics"/>
        </center>

        <div class="box" style="background-color:aliceblue">
          <div class="content has-text-justified">
            <p>
              <b>(Test-Time Self-Training Dynamics)</b> We apply test-time training on AIME 2024 and 
              observe no performance collapse. However, SRT's performance quickly saturates (leftmost plot), 
              and the pseudo-reward value (second plot) also approaches saturation.
            </p>
          </div>
        </div>

        <h2 class="title is-3">Can model collapse during SRT be avoided?</h2>
        
        <p>
        Our initial experiments demonstrate that prolonged training with SRT leads to model collapse due to 
        reward hacking. The next natural question is, can we avoid reward training? This will be crucial in order for
        SRT to be a practical algorithm for improving LLM capabilities.
        </p>

        <div style="height: 20px;"></div>
        <p>
          In this work, we have explored three ideas to avoid or mitigate model collapse, namely: 
        </p>
        <div style="height: 20px;"></div>
        <ul>
          <li><b>(1)</b> An <b>early stopping</b> strategy to avoid model collapse by monitoring the performance on a small labeled validation dataset </li>
          <li><b>(2)</b> An <b>algorithmic</b> strategy that mitigates the risk of model collapse by using pseudo-labels generated from a stable base model rather than from the continuously updated model. </li>
          <li><b>(3)</b> A <b>data-driven</b> curriculum-based strategy to enhance model performance beyond simple early stopping.</li>
        </ul>

        <div style="height: 40px;"></div>

        <h3 class="title is-4">Early stopping lets us retain most of the benefits of SRT</h3>

        The use of a validation dataset for hyper-parameter tuning 
        and model selection is a staple of modern machine learning practice. 
        If a small labeled validation dataset is available, it can be used to identify the peak performance of 
        the model under self-training. As the figure below shows, the exact choice of the validation dataset does not matter much for early stopping.

        <center>
          <img src="./static/images/srt_early_stopping.png"
            alt="srt_early_stopping"
            width="70%"
            class="srt_early_stopping"/>
        </center>

        <div class="box" style="background-color:aliceblue">
          <div class="content has-text-justified">
            <p>
              <b>(Early Stopping is Effective)</b> The peak performance occur at nearly the same point for all heldout sets, 
              so using any would be effective for early stopping.
            </p>
          </div>
        </div>

        <h3 class="title is-4">Generating labels via a fixed checkpoint (offline labeling) instead of the online-evolving policy can provide stability</h3>
        
        Model collapse arises because SRT prioritizes consistency over correctness, 
        increasingly rewarding model agreement even if incorrect. 
        A straightforward yet effective approach to address this involves generating pseudo-labels from a stable, 
        previously fixed checkpoint, rather than leveraging labels from the evolving policy. 
        To evaluate this approach, we generate pseudo-labels via majority voting rollouts 
        from the Qwen2.5-Math-7B base model, store these offline-generated labels, 
        and subsequently perform RL training against them. The figure below demonstrates that 
        training with these offline-generated labels significantly stabilizes training while achieving performance 
        comparable to SRT. Importantly, this indicates that the <strong> dynamic updating of pseudo-labels during training 
        (online labeling) may not always confer substantial benefits</strong>, and instead can contribute to training instability.
        
        <div style="height: 40px;"></div>

        <center>
          <img src="./static/images/srt_offline_generated_data.png"
            alt="srt_offline_generated_data"
            width="100%"
            class="srt_offline_generated_data"/>
        </center>

        <div class="box" style="background-color:aliceblue">
          <div class="content has-text-justified">
            <p>
              <b>(Self-Training with Offline-Generated Labels)</b> Performance comparison between SRT 
              and a baseline variant, where pseudo-labels are precomputed from a fixed base model checkpoint. 
              The offline-generated labels maintain training stability while achieving comparable performance to SRT, 
              highlighting a limitation of the online labeling strategy.
            </p>
          </div>
        </div>

        <h3 class="title is-4">Self-rewarding training with curriculum can avoid (or substantially delay) model collapse</h3>

        <p>
        We hypothesize that <b>model collapse occurs more rapidly when training on more challenging datasets</b>. 
        This conjecture aligns with our empirical findings above: the model experiences earlier collapse 
        when training on the difficult DAPO dataset compared to the simpler MATH-12K dataset. 
        MATH-12k collapses after ~ 1200 steps (2 epochs) of training, while DAPO collapses after ~ 600 steps (1/3 of an epoch) of training.
        </p>
        <div style="height: 40px;"></div>

        <p>
          We leverage this hypothesis to implement a curriculum learning strategy by identifying the 'easiest' subset of 
          the DAPO dataset. To be precise, we retain 1/3-rd of the easiest DAPO prompts selected according to two distinct metrics:
        </p>
        <div style="height: 40px;"></div>
        <ul>
          <li><b>(1)</b> <b>Pass rate of the base model</b>, which utilizes ground-truth labels.
          We sort the questions based on per prompt pass rate out of 32 rollouts from the base model and keep 1/3rd of the prompts with the highest pass rate.
          </li>
          <li><b>(2)</b> <b>Frequency distribution of the majority vote</b>, which does not require ground-truth labels. 
          Similarly, we sort the questions based on the frequency of the majority vote out of 32 rollouts from the base model and keep 1/3rd of the prompts with the highest frequency.
          </li>
        </ul>
        Note that in oth cases the chosen pseudo-label might not be the correct labels, rather just a likely label.

        <div style="height: 40px;"></div>
        <p>
        The Figure below shows that <b>training on these easier subsets significantly delays the onset of reward hacking </b>, 
        allowing for continuous improvement even across multiple epochs. Remarkably, performance on these curriculum subsets 
        reaches levels comparable to standard RL training with ground-truth labels on the entire DAPO dataset.
        </p>

        <div style="height: 40px;"></div>

        <center>
          <img src="./static/images/srt_curriculum.png"
            alt="srt_curriculum"
            width="100%"
            class="srt_curriculum"/>
        </center>

        <div class="box" style="background-color:aliceblue">
          <div class="content has-text-justified">
            <p>
              <b>(Curriculum-Based Self-Training)</b> Performance of SRT on curated subsets 
              containing the easiest 1/3 of prompts from the DAPO dataset, selected based either 
              on model pass rate or frequency of the majority vote. Training on these easier subsets 
              prevents reward hacking even after extensive training (3 epochs), demonstrating the effectiveness 
              of curriculum learning strategies in sustaining continual model improvement.
            </p>
          </div>
        </div>

      </div>
    </div>
  <hr>
  <h2 class="title is-3" id="setup">TL;DR: Takeaways:</h2>
  <div class="content has-text-justified">
    <p>
      In summary:
    </p>          
    <p>  
    <ul>
      <li><b>(1)</b> <b>Can LLMs generate their own supervision for training? If so, can we propose a simple algorithm to achieve this?</b></li>
      <br>&rarr; Yes, we can.
      We propose a simple self-training alogorithm (SRT) that leverages the model's self-consistency to infer correctness signals and train without any ground-truth supervision.
      <li><b>(2)</b> <b>How effective is this self-training algorithm?</b></li>
      <br>&rarr; We show that our simple self training algorithm, SRT, can achieve performance levels comparable to reinforcement learning methods trained explicitly on gold-standard answers on difficult mathematical reasoning tasks.
      We also show that at test time, SRT provides a minor, but noticable boost in performance over several datasets.
      <li><b>(3) Does this method have limitations, and if so, how might we overcome them?</b></li>
      <br>&rarr; Yes, we show that SRT suffers from performance collapse due to reward hacking, where over time, the model learns to produce consistent responses in order to optimize its self-assigned reward, irrespective of their true correctness.
      We explore three strategies to mitigate this issue: (1) early stopping, (2) offline labeling, and (3) curriculum learning (training using easy samples).
      Among them, early stopping and offline labeling are the easiest to implement; however, curriculum learning approach offers insightful into model's learning dynamics.
    </ul>            
  </div>
</section>
</div>

<hr>




<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code> 
      @misc{shafayat2025largereasoningmodelsselftrain,
        title={Can Large Reasoning Models Self-Train?}, 
        author={Sheikh Shafayat and Fahim Tajwar and Ruslan Salakhutdinov and Jeff Schneider and Andrea Zanette},
        year={2025},
        eprint={2505.21444},
        archivePrefix={arXiv},
        primaryClass={cs.LG},
        url={https://arxiv.org/abs/2505.21444}, 
      }
</code></pre>
  </div>
</section>




<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/pdf/2502.17543">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/tajwarfahim/paprika" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <p>
        Corresponding Author: <a href="mailto:sheikhshafayat2@gmail.com">Sheikh Shafayat</a>, <a href="mailto:ftajwar@cs.cmu.edu">Fahim Tajwar</a>, <a href="mailto:zanette@cmu.edu">Andrea Zanette</a>.<br>
        We thank the Nerfies Team for their <a href="https://github.com/nerfies/nerfies.github.io">website template</a>.
      </p>
    </div>
  </div>
</footer>

</body>
</html>
