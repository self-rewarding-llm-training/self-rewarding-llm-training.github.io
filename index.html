<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="A reinforcement learning framework where LLMs generate their own supervision signal via majority voting">
  <meta name="keywords" content="LLM, Reasoning, Self-Improvement">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>srt</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation" style="background-color: #333; color: #fff;">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false" style="color: #fff;">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="#abstract" style="color: #fff; border-bottom: 0px solid #fff;">
        Abstract
      </a>
      <a class="navbar-item" href="#setup" style="color: #fff; border-bottom: 0px solid #fff;">
        Problem Setup
      </a>
      <a class="navbar-item" href="#srt" style="color: #fff; border-bottom: 0px solid #fff;">
        SRT
      </a>
      <a class="navbar-item" href="#empirical" style="color: #fff; border-bottom: 0px solid #fff;">
        Empirical Results
      </a>
      <a class="navbar-item" href="#BibTeX" style="color: #fff; border-bottom: 0px solid #fff;">
        BibTeX
      </a>
  </div>
</nav>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Can Large Reasoning Models Self-Train?</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://sheikhshafayat.github.io/">Sheikh Shafayat</a><sup>*1</sup>,
            </span>
            <span class="author-block">
              <a href="https://tajwarfahim.github.io/">Fahim Tajwar</a><sup>*2</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.cs.cmu.edu/~rsalakhu/">Ruslan Salakhutdinov</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.cs.cmu.edu/~schneide/">Jeff Schneider</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://azanette.com">Andrea Zanette</a><sup>2</sup>
            </span>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>*</sup>Equal Contribution</span>
            <span class="author-block"><sup>1</sup>Independent Researcher</span>
            <span class="author-block"><sup>2</sup>Carnegie Mellon University</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/abs/2502.17543"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/tajwarfahim/srt"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <span class="link-block">
                <a href="https://huggingface.co/collections/ftajwar/self-rewarding-llm-training-6835218091832c3664176553"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-database"></i>
                  </span>
                  <span>Datasets</span>
                  </a>
              </span>

          </div>
        </div>
      </div>
    </div>
  </div>

</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/srt_teaser_figure.png"
        alt="Teaser image."
        class="teaser-image"/>
      <h2 class="subtitle has-text-centered">
        <b>(Overview of Self-Rewarding Training or SRT)</b> In Reinforcement Learning from Verifiable Reward (RLVR), one produces the reward for RL training using a ground truth verifier. 
        Contrary to that, Self-Rewarding Training (SRT) does not assume access to a ground truth verifier; instead it uses majority voting 
        from the model's own generations to estimate the ground truth, and use this proxy reward signal to train the model.
      </h2>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3" id="abstract">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Scaling the performance of large language models (LLMs) increasingly depends on 
            methods that reduce reliance on human supervision. 
            Reinforcement learning from automated verification offers an alternative, but it incurs scalability limitations 
            due to dependency upon human-designed verifiers. Self-training, where the model’s own judgment provides the 
            supervisory signal, presents a compelling direction. 
            We propose an online self-training reinforcement learning algorithm that leverages the model’s self-consistency 
            to infer correctness signals and train without any ground-truth supervision. 
            We apply the algorithm to challenging mathematical reasoning tasks and show that it quickly reaches performance 
            levels rivaling reinforcement-learning methods trained explicitly on gold-standard answers. 
            Additionally, we analyze inherent limitations of the algorithm, highlighting how the self-generated proxy reward 
            initially correlated with correctness can incentivize reward hacking, where confidently 
            incorrect outputs are favored. Our results illustrate how self-supervised improvement can achieve significant performance 
            gains without external labels, while also revealing its fundamental challenges.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  <hr>
<section class="section">
  <div class="container is-max-desktop">
    <!-- Animation. -->
    <div class="columns is-centered">
      <div class="column is-full-width">

        <h2 class="title is-3" id="setup">Problem Setting</h2>
        <div class="content has-text-justified">
          <p>
            Recently reinforcement learning from verifiable rewards (RLVR) have seen a lot of success at improving
            LLMs' reasoning abilities, particularly in the domains like math and coding (<a href="https://arxiv.org/abs/2412.16720">OpenAI et al.</a>,
            <a href="https://arxiv.org/abs/2501.12948">DeepSeek-AI et al.</a>). 
            However, these approaches 
            require ground truth (possibly human written) verifiers to provide reward signal during training. This can
            be quite limiting, since collecting gold answers for every problem we care about can become expensive.
          </p>

          <p>
            The goal of our paper is to answer the following questions: 
          </p>
          <ul>
            <li><b>(1)</b> Can LLMs generate their own training supervision? If so, can we provide a simple algorithm for this?</li>
            <li><b>(2)</b> How does this algorithm for self-training perform?</li>
            <li><b>(3)</b> Do this method have any limitations? How can we address them?</li>
          </ul>
        </div>

        <h2 class="title is-3" id="srt">Self-Rewarding Training (SRT)</h2>
        <div class="content has-text-justified">
          In our work, we provide a simple algorithm for self-training. We notice that LLMs are generally more consistent when
          they are correct (for example, majority voting typically outperforms average performance). We leverage this property
          to generate training signal for an LLM wihout ground truth labels --- concretely, during RL training, we calculate
          the majority voting answer for each prompt, and use this as if it was the ground truth answer. This is a simple algorithm
          compatible with any RLVR algorithm since the only change is how we generate the reward signal. Next, we describe different
          components of SRT.
        </div>

        <center>
          <img src="./static/images/srt_components.png"
            width="60%"
            alt="srt_components"
            class="srt_components"/>
        </center>

        <div class="box" style="background-color:aliceblue">
          <div class="content has-text-centered">
            <p>
              <b>(Different Components of SRT)</b> The SRT algorithm consists of taking an prompt or question,
              generating multiple responses from the LLM for the prompt, calculate the majority voting answer based on these
              responses, and then use the majority voting answer as a proxy for ground truth to provide reward for RL training.
            </p>
          </div>
        </div>
        
        <h3 class="title is-4">Training Algorithm</h3>
        <div class="content has-text-justified">
          <!-- Create Bullet Point List to Summarize -->
          We provide our formal training algorithm below:
        </div>

        <center>
          <img src="./static/images/srt_algorithm.png"
            width="50%"
            alt="srt_algorithm"
            class="srt_algorithm"/>
        </center>        

        <h2 class="title is-3" id="empirical">Empirical Results</h2>

        <!-- Interpolating. -->
        <h3 class="title is-4">SRT leads to 100% relative improvement, but also suffers from performance collapse</h3>
        
        <center>
          <img src="./static/images/self_training_performance.png"
            alt="self_training_performance"
            width="90%"
            class="self_training_performance"/>
        </center>

        <div class="box" style="background-color:aliceblue">
          <div class="content has-text-centered">
            <p>
              <b>(Self-Training Performance)</b> Average over 3 heldout test dataset performance of SRT vs training with ground truth reward. 
              Training for longer with SRT can lead to performance collapse, but at the peak, it leads to around 100% performance improvement over the base model.
              Earlier in the training, it leads to similar improvement as RL training with ground truth.
            </p>
          </div>
        </div>

        <h3 class="title is-4">What happens after SRT peaks in performance?</h3>

        Prolonged training with SRT can lead to <b>reward hacking</b> ---the model 
        learns to produce consistent responses in order to optimize its self-assigned reward, 
        irrespective of their true correctness. Indeed, manual analysis of the model outputs 
        (examples shown below) confirms this hypothesis: after collapse, the model outputs a very high entropy, 
        essentially random, set of tokens followed by the same template final answer that is nearly independent of the input prompt.
        In other words, <b>the initially strong correlation between the SRT objective and correctness is ultimately compromised, 
        becoming no longer a reliable proxy signal.</b>
        
        <center>
          <img src="./static/images/srt_reward_hacking_example.png"
            alt="srt_reward_hacking_example"
            width="90%"
            class="srt_reward_hacking_example"/>
        </center>

        <div class="box" style="background-color:aliceblue">
          <div class="content has-text-centered">
            <p>
              <b>(Model response for the same prompts before and after training with SRT)</b> Three examples of model responses 
              for the same prompt, before and after prolonged training with SRT on the DAPO dataset, 
              for a Qwen2.5-Math-7B model. Notice that for some prompts, the model responses before training ends before completion, 
              this is due to the model running out of our token generation budget. The model after 1200 steps of SRT training 
              exhibits performance collapse, and it outputs \texttt{\string\boxed\{1\}} and some other incoherent set of tokens 
              irrespective of the given prompt.
            </p>
          </div>
        </div>

        <center>
          <img src="./static/images/srt_training_dynamics.png"
            alt="srt_training_dynamics"
            width="100%"
            class="srt_training_dynamics"/>
        </center>

        <div class="box" style="background-color:aliceblue">
          <div class="content has-text-centered">
            <p>
              <b>(Self-Training Dynamics)</b> Extended training via SRT can lead to reward hacking, 
              as demonstrated by the sudden hike in KL penalty and training (psuedo-)reward, but collapse of accuracy on the held-out test sets.
            </p>
          </div>
        </div>
  
        <h3 class="title is-4">SRT can be used for test-time training</h3>

        Since SRT does not require ground truth answers, one can also use our algorithm for test-time training ---
        that is, given a test set, one can first run SRT on this set, and then make predictions. We
        see that this can lead to minor
        performance improvement on the test set.

        <center>
          <img src="./static/images/srt_test_time_training.png"
            alt="srt_test_time_training"
            width="90%"
            class="srt_test_time_training"/>
        </center>
        
        <div class="box" style="background-color:aliceblue">
          <div class="content has-text-justified">
            <p>
              <b>(Test-Time Self-Training Performance)</b> Given the test dataset, 
              one can perform SRT on this dataset before making predictions. Our results show that this 
              improves the majority voting performance on the test set without access to ground truth labels.
            </p>
          </div>
        </div>

        Moreover, given the test dataset is small enough, SRT pseudo-reward can very quickly saturate.
        This means very little learning signal is provided to the model throughout the rest of the training. As a result,
        we observe very little reward hacking or performance collapse in the test-time training setup, compared to the regular setup.

        <center>
          <img src="./static/images/srt_test_time_training_dynamics.png"
            alt="srt_test_time_training_dynamics"
            width="90%"
            class="srt_test_time_training_dynamics"/>
        </center>

        <div class="box" style="background-color:aliceblue">
          <div class="content has-text-justified">
            <p>
              <b>(Test-Time Self-Training Dynamics)</b> We apply test-time training on AIME 2024 and 
              observe no performance collapse. However, SRT's performance quickly saturates (leftmost plot), 
              and the pseudo-reward value (second plot) also approaches saturation.
            </p>
          </div>
        </div>

        <h2 class="title is-4">Can model collapse during SRT be avoided?</h2>
        
        <p>
        Our initial experiments demonstrate that prolonged training with SRT leads to model collapse due to 
        reward hacking. The next natural question is, can we avoid reward training? This will be crucial in order for
        SRT to be a practical algorithm for improving LLM capabilities.
        </p>

        <p>
          In this work, we have explored three ideas to avoid or mitigate model collapse, namely: 
        </p>
        <ul>
          <li><b>(1)</b> An <b>early stopping</b> strategy to avoid model collapse by monitoring the performance on a small labeled validation dataset </li>
          <li><b>(2)</b> An <b>algorithmic</b> strategy that mitigates the risk of model collapse by using pseudo-labels generated from a stable base model rather than from the continuously updated model. </li>
          <li><b>(3)</b> A <b>data-driven</b> curriculum-based strategy to enhance model performance beyond simple early stopping.</li>
        </ul>

        <p>
          
        </p>

        <h3 class="title is-4">Early stopping lets us retain most of the benefits of SRT</h3>

        The use of a validation dataset for hyper-parameter tuning 
        and model selection is a staple of modern machine learning practice. 
        If a small labeled validation dataset is available, it can be used to identify the peak performance of 
        the model under self-training.

        <center>
          <img src="./static/images/srt_early_stopping.png"
            alt="srt_early_stopping"
            width="70%"
            class="srt_early_stopping"/>
        </center>

        <div class="box" style="background-color:aliceblue">
          <div class="content has-text-justified">
            <p>
              <b>(Early Stopping is Effective)</b> The peak performance occur at nearly the same point for all heldout sets, 
              so using any would be effective for early stopping.
            </p>
          </div>
        </div>

        <h3 class="title is-4">Generating labels via a fixed checkpoint instead of the online-evolving policy can provide stability</h3>
        
        The tendency toward model collapse arises because SRT prioritizes consistency over correctness, 
        increasingly rewarding model agreement even if incorrect. 
        A straightforward yet effective approach to address this involves generating pseudo-labels from a stable, 
        previously fixed checkpoint, rather than leveraging labels from the evolving policy. 
        To evaluate this approach, we generate pseudo-labels via majority voting rollouts 
        from the Qwen2.5-Math-7B base model, store these offline-generated labels, 
        and subsequently perform RL training against them. The figure below demonstrates that 
        training with these offline-generated labels significantly stabilizes training while achieving performance 
        comparable to SRT. Importantly, this indicates that the dynamic updating of pseudo-labels during training 
        (online labeling) may not always confer substantial benefits, and instead can contribute to training instability.
        
        <center>
          <img src="./static/images/srt_offline_generated_data.png"
            alt="srt_offline_generated_data"
            width="70%"
            class="srt_offline_generated_data"/>
        </center>

        <div class="box" style="background-color:aliceblue">
          <div class="content has-text-justified">
            <p>
              <b>(Self-Training with Offline-Generated Labels)</b> Performance comparison between SRT 
              and a baseline variant, where pseudo-labels are precomputed from a fixed base model checkpoint. 
              The offline-generated labels maintain training stability while achieving comparable performance to SRT, 
              highlighting a limitation of the online labeling strategy.
            </p>
          </div>
        </div>

        <h3 class="title is-4">Self-rewarding training with curriculum can avoid (or substantially delay) model collapse</h3>

        <p>
        We hypothesize that <b>model collapse occurs more rapidly when training on more challenging datasets</b>. 
        This conjecture aligns with our empirical findings above: the model experiences earlier collapse 
        when training on the difficult DAPO dataset compared to the simpler MATH-12K dataset. 
        </p>

        <p>
          We leverage this hypothesis to implement a curriculum learning strategy by identifying the 'easiest' subset of 
          the DAPO dataset. To be precise, we retain 1/3-rd of the easiest DAPO prompts selected according to two distinct metrics:
        </p>

        <ul>
          <li><b>(1)</b> <b>Pass rate of the base model</b>, which utilizes ground-truth labels. </li>
          <li><b>(2)</b> <b>Frequency of the majority vote</b>, which does not require ground-truth labels. </li>
        </ul>

        <p>
        The Figure below shows that training on these easier subsets significantly delays the onset of reward hacking, 
        allowing for continuous improvement even across multiple epochs. Remarkably, performance on these curriculum subsets 
        reaches levels comparable to standard RL training with ground-truth labels on the entire DAPO dataset.
        </p>


        <center>
          <img src="./static/images/srt_curriculum.png"
            alt="srt_curriculum"
            width="70%"
            class="srt_curriculum"/>
        </center>

        <div class="box" style="background-color:aliceblue">
          <div class="content has-text-justified">
            <p>
              <b>(Curriculum-Based Self-Training)</b> Performance of SRT on curated subsets 
              containing the easiest 1/3 of prompts from the DAPO dataset, selected based either 
              on model pass rate or frequency of the majority vote. Training on these easier subsets 
              prevents reward hacking even after extensive training (3 epochs), demonstrating the effectiveness 
              of curriculum learning strategies in sustaining continual model improvement.
            </p>
          </div>
        </div>

      </div>
    </div>
  </div>
</section>

<hr>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code> 
      @misc{shafayat2025CanLargeReasoningModelsSelfTrain,
        title={Can Large Reasoning Models Self-Train?}, 
        author={Sheikh Shafayat and Fahim Tajwar and Ruslan Salakhutdinov and Jeff Schneider and Andrea Zanette},
        year={2025},
        url={https://self-rewarding-llm-training.github.io/}, 
      }
</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/pdf/2502.17543">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/tajwarfahim/paprika" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <p>
        Corresponding Author: <a href="mailto:sheikhshafayat2@gmail.com">Sheikh Shafayat</a>, <a href="mailto:ftajwar@cs.cmu.edu">Fahim Tajwar</a>, <a href="mailto:zanette@cmu.edu">Andrea Zanette</a>.<br>
        We thank the Nerfies Team for their <a href="https://github.com/nerfies/nerfies.github.io">website template</a>.
      </p>
    </div>
  </div>
</footer>

</body>
</html>
