<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="A reinforcement learning framework where LLMs generate their own supervision signal via majority voting">
  <meta name="keywords" content="LLM, Reasoning, Self-Improvement">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>SRT: Can Large Reasoning Models Self-Train?</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Can Large Reasoning Models Self-Train?</h1>
          
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://sheikhshafayat.github.io/">Sheikh Shafayat</a><sup>1*</sup>,
            </span>
            <span class="author-block">
              <a href="https://tajwarfahim.github.io/">Fahim Tajwar</a><sup>2*</sup>,
            </span><br>
            <span class="author-block">
              <a href="https://www.cs.cmu.edu/~rsalakhu/">Ruslan Salakhutdinov</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.cs.cmu.edu/~schneide/">Jeff Schneider</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://azanette.com">Andrea Zanette</a><sup>2</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup> KAIST</span>
            <span class="author-block" style="margin-left: 1em;"><sup>2</sup> Carnegie Mellon University</span><br>
            <span class="author-block"><sup>*</sup> Equal Contribution</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/abs/2505.21444"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/tajwarfahim/srt"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <span class="link-block">
                <a href="https://huggingface.co/collections/ftajwar/self-rewarding-llm-training-6835218091832c3664176553"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-database"></i>
                  </span>
                  <span>Datasets</span>
                  </a>
              </span>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Animation. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="content has-text-justified">
        <p>
          In this work, we ask the following question: <b>can self-training — the process where a language model learns from its own judgments — can be sustained within a RL framework?</b> 
          <br><br>
          On a comprehensive set of experiments on both synthetic and real reasoning tasks, we find that this basic approach has the capability to improve not only the model's reasoning performance, but also its potential of <b>generating better quality feedback for the next RL iteration</b>, driving further model improvement. Yet our analysis also reveals a critical limitation of such a self-training paradigm — prolonged RL with self-reward leads to reward hacking where models learn to maximize training (pseudo-)reward, resulting in sudden performance collapse.
          <br><br>
          Together, these results highlight feedback design as the central challenge and call for future research on mechanisms to enable prolonged self-improvement.
         
        </div>
        
        <br><br>

        <h2 class="title is-3" id="srt">(1) Self-Rewarded Training (SRT)</h2>
        <div class="content has-text-justified">
          In this work, we take the simplest possible self reward mechanism, majority voting, and experiment with a simple yet effective self-training method, titled Self-Rewarded Training (SRT). SRT uses consistency across multiple model-generated solutions to estimate correctness during RL training, providing self-supervision signals without labeled data.
            <!-- Motivated by prior works based on consistency based self-improvement, we introduce a simple yet effective self-training reinforcement learning methodology, Self-Rewarded Training, that uses consistency across multiple model-generated solutions to estimate correctness during RL training, providing self-supervision signals without labeled data. -->
        </div>

        <!-- <center>
          <img src="./static/images/srt_components.png"
            width="60%"
            alt="srt_components"
            class="srt_components"/>
        </center> -->
        <section class="hero teaser">
          <div class="container is-max-desktop">
            <div class="hero-body">
              <img src="./static/images/srt_teaser_figure.png"
                alt="Teaser image."
                class="teaser-image"/>
            </div>
          </div>
        </section>

        <br><br>

        <h2 class="title is-3" id="srt">(2) Self Training Can Improve Both Performance <b>and</b> Feedback Quality</h2>
        <div class="content has-text-justified">
        
      </div>

        <!-- Interpolating. -->
        <!-- <h3 class="title is-4">SRT leads to 100% relative improvement, but also suffers from performance collapse</h3> -->
        
        <center>
          <img src="./static/images/reasoning_gym.png"
            alt="Self training performance on reasoning gym datasets."
            width="90%"
            class="self_training_performance"/>
        </center>

        <div class="content has-text-justified">
      We investigate self-training under controlled settings on synthetic reasoning tasks from Reasoning Gym. Remarkably, SRT improves not only the mean accuracy, but the majority voting accuracy as well, which is the source of our training supervision. Improvement in the quality of training signal drives further improvement in performance, as SRT outperforms its variant employing the majority votes from a fixed teacher as proxy labels.
      <br><br>
      To ensure the model understood the task and output format, we first trained it with RL using ground-truth data from the previous difficulty level before proceeding to label-free SRT training.
      </div>
        
      <br><br>

        <h2 class="title is-3" id="srt">(2) SRT Works on Real-World Reasoning Tasks</h2>
        <div class="content has-text-justified">
        We empirically demonstrate that at early training stages, SRT achieves performance rivaling standard reinforcement-learning methods trained explicitly on gold-standard answers.       
        Test datasets: AMC, AIME24, AIME25.
      </div>

        <!-- Interpolating. -->
        <!-- <h3 class="title is-4">SRT leads to 100% relative improvement, but also suffers from performance collapse</h3> -->
        
        <center>
          <img src="./static/images/self_training_performance.png"
            alt="self_training_performance"
            width="90%"
            class="self_training_performance"/>
        </center>

        <div class="content has-text-justified">
      However, we find that the performance eventually collapses, see e.g. the trainining on DAPO in the rightmost picture---more on this below.
      </div>
        
      <br><br>
      
        <h2 class="title is-3" id="srt">(3) Self-Training is Bound to Collapse</h2>
        <div class="content has-text-justified">
        We analyze the training dynamics of SRT when training on the challenging DAPO dataset.
      </div>


        <center>
          <img src="./static/images/srt_training_dynamics.png"
            alt="srt_training_dynamics"
            width="100%"
            class="srt_training_dynamics"/>
        </center>

        <div class="content has-text-justified">
These findings indicate that the model learns to maximize self-assigned rewards by producing consistent (second graph above) but incorrect answers (leftmost graph above). 
Manual inspection confirms this: after collapse, model outputs degenerate into random token sequences with a fixed, prompt-independent answer (e.g., 'the solution is 1'). 
        There is a simple yet precise theoretical justification for this behavior:      
        </div>       

          <div class="box" style="background-color:aliceblue">
          <div class="content has-text-centered">
        <p>
          <b>The reinforcement learning optimization problem defined by the SRT objective<br>explicitly encourages consistency across outputs, independently of correctness.</b>
        </p>
          </div>
        </div>
        <div class="content has-text-justified">
        Thus, the optimal policy under this objective degenerates to producing the same answer regardless of input, maximizing reward artificially. 
        Continued self-training on this proxy naturally drives the model toward this trivial solution, especially when it's simpler than solving the actual task.        </div>  

        <br><br>

        <h2 class="title is-3" id="srt">(4) Mitigation Strategies can be Effective</h2>
        <div class="content has-text-justified">

<p>We propose strategies to mitigate reward hacking, laying the groundwork for effective future
approaches to sustaining continual model improvement.</p>

<br>

<p><strong>(i) Early Stopping:</strong> A small validation set can reliably detect peak model performance and prevent collapse during self-training. 
The peak performance occurs at nearly the same point for all heldout sets, so using any would be effective for early stopping.</p>

        <center>
          <img src="./static/images/srt_early_stopping.png"
            alt="srt_early_stopping"
            width="70%"
            class="srt_early_stopping"/>
        </center>

        <br><br>

<p><strong>(ii) Self-Training with Offline-Generated Labels:</strong> An effective approach involves generating pseudo-labels from a stable, previously
fixed checkpoint, rather than leveraging labels from the evolving policy. Doing so stabilizes training while achieving performance comparable to SRT. </p>

        <center>
          <img src="./static/images/srt_offline_generated_data.png"
            alt="srt_offline_generated_data"
            width="100%"
            class="srt_offline_generated_data"/>
        </center>

        <br><br>

        <p><strong>(iii) Self-Training with Curriculum Learning:</strong> We hypothesize that model collapse occurs more rapidly when training on more challenging datasets, 
a conjecture that aligns with our empirical findings. The intuition is that, on a more challenging dataset, the model finds it easier to abandon its pre-trained knowledge in favor of optimizing self-consistency rather than genuinely learning to solve the
underlying task. We leverage this hypothesis to implement a curriculum learning strategy by identifying the ‘easiest’ subset of the DAPO dataset according to (a) the pass rate and (b) the frequency of the majority vote (see paper for more details).  </p>

        <center>
          <img src="./static/images/srt_curriculum.png"
            alt="srt_curriculum"
            width="100%"
            class="srt_curriculum"/>
        </center>

        Performance on
these curriculum subsets reaches levels comparable to standard RL training with ground-truth labels
on the entire DAPO dataset. These promising results suggest that curriculum strategies may further
extend the benefits of SRT, opening exciting avenues for future investigation.

        <br><br>
        
  <hr>
  <h2 class="title is-3" id="setup">Related Literature:</h2>
  <div class="content has-text-justified">
    <p> Several works explore self-training LLMs, some of which are concurrent. A non-exaustive list includes:
      <ul>
        <li><a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/639a9a172c044fbb64175b5fad42e9a5-Abstract-Conference.html">Zeikerman et al. (2022)</a> and <a href="https://arxiv.org/abs/2210.11610">Huang et al (2022)</a>: demonstrates
        that LLMs can self-improve by training on chain-of-thoughts from a previous instance of the model. Particularly, 
        <a href="https://arxiv.org/abs/2210.11610" target="_blank">Huang et al., 2023</a>; 
        <a href="https://arxiv.org/abs/2203.11171" target="_blank">Wang et al., 2023a</a>; 
        <a href="https://arxiv.org/abs/2411.04109" target="_blank">Prasad et al., 2024</a> 
        demonstrated the feasibility of using majority voting and self-consistency to filter
        chain-of-thought traces that, when used as SFT training data, improve the LLM performance on
        downstreaming tasks.</li>
        <li><a href="https://arxiv.org/abs/2411.04109" target="_blank">TTRL (Zuo et al., 2025)</a>, 
          proposes a test-time training algorithm, which is equivalent to SRT used at test time. 
          </li>
        <li>Similarly, <a href="https://arxiv.org/abs/2505.19590" target="_blank">Zhao et al., 2025</a> incorporates token certainty as a training signal.</li>
        <li> 
          <a href="https://github.com/ruixin31/Rethink_RLVR/blob/main/paper/rethink-rlvr.pdf" target="_blank">Shao et al., 2025</a> 
          shows that RL with spurious reward can lead to a performance boost for some LLMs.
        </li>
      </ul>
      
    </p>          

  </div>
</section>
</div>

<hr>




<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code> 
      @misc{shafayat2025largereasoningmodelsselftrain,
        title={Can Large Reasoning Models Self-Train?}, 
        author={Sheikh Shafayat and Fahim Tajwar and Ruslan Salakhutdinov and Jeff Schneider and Andrea Zanette},
        year={2025},
        eprint={2505.21444},
        archivePrefix={arXiv},
        primaryClass={cs.LG},
        url={https://arxiv.org/abs/2505.21444}, 
      }
</code></pre>
  </div>
</section>




<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/abs/2505.21444">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/tajwarfahim/srt" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <p>
        Corresponding Author: <a href="mailto:sheikhshafayat2@gmail.com">Sheikh Shafayat</a>, <a href="mailto:ftajwar@cs.cmu.edu">Fahim Tajwar</a>, <a href="mailto:zanette@cmu.edu">Andrea Zanette</a>.<br>
        We thank the Nerfies Team for their <a href="https://github.com/nerfies/nerfies.github.io">website template</a>.
      </p>
    </div>
  </div>
</footer>

</body>
</html>
